{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import supersuit as ss\n",
    "from stable_baselines3 import PPO, DQN\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "from pettingzoo.sisl import multiwalker_v9\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\n",
    "\n",
    "\n",
    "logdir = \"logs\"\n",
    "\n",
    "\n",
    "\n",
    "def train_butterfly_supersuit(\n",
    "    env_fn, steps: int = 10_000, seed: int | None = 0, **env_kwargs\n",
    "):\n",
    "    # Train a single model to play as each agent in a cooperative Parallel environment\n",
    "    env = env_fn.parallel_env(**env_kwargs, n_walkers = 2)\n",
    "\n",
    "    env.reset(seed=seed)\n",
    "\n",
    "    print(f\"Starting training on {str(env.metadata['name'])}.\")\n",
    "    \n",
    "    env = ss.frame_stack_v1(env, 3)\n",
    "    env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "    env = ss.concat_vec_envs_v1(env, 8, num_cpus=2, base_class=\"stable_baselines3\")\n",
    "    \n",
    "\n",
    "    model = PPO(\n",
    "        MlpPolicy,\n",
    "        env,\n",
    "        verbose=2,\n",
    "        learning_rate=2.5e-4,\n",
    "        batch_size=256,\n",
    "        normalize_advantage=True,\n",
    "        n_steps=2048,\n",
    "        n_epochs=10,\n",
    "        gae_lambda=0.95,\n",
    "        gamma=0.99,\n",
    "        clip_range=0.2,\n",
    "        ent_coef=0.001,\n",
    "        tensorboard_log=logdir,\n",
    "    )\n",
    "\n",
    "    # Use deterministic actions for evaluation\n",
    "    callback = EvalCallback(env)\n",
    "\n",
    "    model.learn(total_timesteps=steps, callback=callback)\n",
    "\n",
    "    model.save(f\"{env.unwrapped.metadata.get('name')}_{time.strftime('%Y%m%d-%H%M%S')}\")\n",
    "\n",
    "    print(\"Model has been saved.\")\n",
    "\n",
    "    print(f\"Finished training on {str(env.unwrapped.metadata['name'])}.\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "def eval(env_fn, num_games: int = 100, render_mode: str | None = None, **env_kwargs):\n",
    "    # Evaluate a trained agent vs a random agent\n",
    "    env = env_fn.env(render_mode=render_mode, **env_kwargs, n_walkers = 2)\n",
    "    \n",
    "    # Apply the same frame stacking to the evaluation environment\n",
    "    env = ss.frame_stack_v1(env, 3)\n",
    "    \n",
    "\n",
    "    print(\n",
    "        f\"\\nStarting evaluation on {str(env.metadata['name'])} (num_games={num_games}, render_mode={render_mode})\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        latest_policy = max(\n",
    "            glob.glob(f\"{env.metadata['name']}*.zip\"), key=os.path.getctime\n",
    "        )\n",
    "    except ValueError:\n",
    "        print(\"Policy not found.\")\n",
    "        exit(0)\n",
    "\n",
    "    model = PPO.load(latest_policy)\n",
    "\n",
    "    rewards = {agent: 0 for agent in env.possible_agents}\n",
    "\n",
    "    # Note: We train using the Parallel API but evaluate using the AEC API\n",
    "    # SB3 models are designed for single-agent settings, we get around this by using the same model for every agent\n",
    "    for i in range(num_games):\n",
    "        env.reset(seed=i)   \n",
    "\n",
    "        for agent in env.agent_iter():\n",
    "            \n",
    "            obs, reward, termination, truncation, info = env.last()\n",
    "\n",
    "            for a in env.agents:\n",
    "                rewards[a] += env.rewards[a]\n",
    "            if termination or truncation:\n",
    "                break\n",
    "            else:\n",
    "                act = model.predict(obs, deterministic=True)[0]\n",
    "\n",
    "            env.step(act)\n",
    "    env.close()\n",
    "\n",
    "    avg_reward = sum(rewards.values()) / len(rewards.values())\n",
    "    print(\"Rewards: \", rewards)\n",
    "    print(f\"Avg reward: {avg_reward}\")\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_fn = multiwalker_v9\n",
    "env_kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_butterfly_supersuit(env_fn, **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteo/projects/RL/RL-Projects/multiwalker/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import time\n",
    "\n",
    "import optuna\n",
    "import supersuit as ss\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "from pettingzoo.sisl import multiwalker_v9\n",
    "\n",
    "logdir = \"logs\"\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256, 512])\n",
    "\n",
    "    env = multiwalker_v9.parallel_env(n_walkers=2)\n",
    "    env = ss.frame_stack_v1(env, 3)\n",
    "    env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "    env = ss.concat_vec_envs_v1(env, 8, num_cpus=2, base_class=\"stable_baselines3\")\n",
    "\n",
    "    model = PPO(MlpPolicy,\n",
    "                env,\n",
    "                verbose=1,\n",
    "                learning_rate=learning_rate,\n",
    "                batch_size=batch_size,\n",
    "                normalize_advantage=True,\n",
    "                n_steps=2048,\n",
    "                n_epochs=10,\n",
    "                gae_lambda=0.95,\n",
    "                gamma=0.99,\n",
    "                clip_range=0.2,\n",
    "                ent_coef=0.001,\n",
    "                )\n",
    "\n",
    "    model.learn(total_timesteps=10000, callback=EvalCallback())\n",
    "    return model.ep_info_buffer[0]['r']\n",
    "\n",
    "def train_with_optuna(trial, env_fn):\n",
    "    logdir = \"logs\"\n",
    "\n",
    "    env = env_fn.parallel_env(n_walkers=2)\n",
    "    env = ss.frame_stack_v1(env, 3)\n",
    "    env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "    env = ss.concat_vec_envs_v1(env, 8, num_cpus=2, base_class=\"stable_baselines3\")\n",
    "\n",
    "    model = PPO(\n",
    "        MlpPolicy,\n",
    "        env,\n",
    "        verbose=2,\n",
    "        learning_rate=trial.suggest_loguniform('learning_rate', 1e-5, 1e-1),\n",
    "        batch_size=trial.suggest_categorical('batch_size', [32, 64, 128, 256, 512]),\n",
    "        normalize_advantage=True,\n",
    "        n_steps=2048,\n",
    "        n_epochs=10,\n",
    "        gae_lambda=0.95,\n",
    "        gamma=0.99,\n",
    "        clip_range=0.2,\n",
    "        ent_coef=0.001,\n",
    "        tensorboard_log=logdir,\n",
    "    )\n",
    "\n",
    "    # Use deterministic actions for evaluation\n",
    "    callback = EvalCallback(env)\n",
    "\n",
    "    model.learn(total_timesteps=10000, callback=callback)\n",
    "    \n",
    "    # Access the last episode reward directly from the callback\n",
    "    last_episode_reward = callback.locals['infos'][-1]['episode']['r']\n",
    "    \n",
    "    print(f\"Last episode reward: {last_episode_reward}\")\n",
    "\n",
    "def optimize_hyperparameters(env_fn):\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    optimize = lambda trial: train_with_optuna(trial, env_fn)\n",
    "\n",
    "    study.optimize(optimize, n_trials=100)\n",
    "\n",
    "    print('Number of finished trials: ', len(study.trials))\n",
    "    print('Best trial:')\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print('Value: ', trial.value)\n",
    "    print('Params: ')\n",
    "    for key, value in trial.params.items():\n",
    "        print(f'    {key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "optimize_hyperparameters(multiwalker_v9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_butterfly_supersuit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting evaluation on multiwalker_v9 (num_games=2, render_mode=human)\n",
      "Rewards:  {'walker_0': 115.35965141902366, 'walker_1': 115.35965141902366}\n",
      "Avg reward: 115.35965141902366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "115.35965141902366"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(env_fn, num_games=2, render_mode=\"human\", **env_kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
